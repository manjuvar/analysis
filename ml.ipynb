To distinguish between different categories like "terrorism" and "natural disaster" using NLTK, you would need to implement a more advanced text classification system. NLTK's Sentiment Intensity Analyzer (VADER) is generally used for sentiment analysis (positive/negative) and is not suitable for categorizing content into specific themes like "terrorism" or "natural disaster."

To categorize text into specific themes, you would typically need to:

1. **Collect a Labeled Dataset:** Gather a set of texts that are labeled as "terrorism," "natural disaster," or other relevant categories.

2. **Feature Extraction:** Extract features from the texts that are useful for classification. This could be simple features like word counts or more complex features like TF-IDF (Term Frequency-Inverse Document Frequency).

3. **Train a Classifier:** Use the labeled dataset to train a classification model. Common algorithms include Naive Bayes, Support Vector Machines (SVM), or more advanced methods like neural networks.

4. **Classify New Texts:** Use the trained model to classify new texts into the predefined categories.

Here is an example of how you might start with a simple Naive Bayes classifier:

```python
import nltk
from nltk.corpus import movie_reviews
from random import shuffle
from nltk.classify import NaiveBayesClassifier

# You would replace the movie_reviews corpus with your own labeled data
documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]
shuffle(documents)

# Feature extraction: here, we simply use word presence as features
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

# Train the Naive Bayes classifier
featuresets = [(document_features(d), c) for (d, c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = NaiveBayesClassifier.train(train_set)

# Test the classifier
print(nltk.classify.accuracy(classifier, test_set))
```

Note: This is a basic example using the `movie_reviews` dataset. For your application, you need to create a dataset with texts labeled as "terrorism," "natural disaster," etc. The process involves significant effort in data preparation and model tuning.

import feedparser

# RSS feeds
rss_feeds = ["https://cms.qz.com/feed/", "https://feeds.feedburner.com/NewshourWorld", 'https://feeds.bbci.co.uk/news/world/asia/india/rss.xml']
rss = "https://feeds.feedburner.com/NewshourWorld"
# Database connection details


feed = feedparser.parse(rss)

articles_list = []

for entry in feed.entries:
    title = entry.title
    link = entry.link
    description = entry.description
    content = entry.content[0]['value']
    published_date = entry.published
    guid = entry.guid

    article = {
    'Title': title,
    'URL': link,
    'Description': description,
    'Content': content,
    'Published Date': published_date,
    'GUID': guid
    }
    articles_list.append(article)
   
